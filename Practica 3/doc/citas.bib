@InProceedings{main-article,
	author="Fernandes, Kelwin
		and Vinagre, Pedro
		and Cortez, Paulo",
	editor="Pereira, Francisco
		and Machado, Penousal
		and Costa, Ernesto
		and Cardoso, Am{\'i}lcar",
	title="A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News",
	booktitle="Progress in Artificial Intelligence",
	year="2015",
	publisher="Springer International Publishing",
	address="Cham",
	pages="535--546",
	abstract="Due to the Web expansion, the prediction of online news popularity is becoming a trendy research topic. In this paper, we propose a novel and proactive Intelligent Decision Support System (IDSS) that analyzes articles prior to their publication. Using a broad set of extracted features (e.g., keywords, digital media content, earlier popularity of news referenced in the article) the IDSS first predicts if an article will become popular. Then, it optimizes a subset of the articles features that can more easily be changed by authors, searching for an enhancement of the predicted popularity probability. Using a large and recently collected dataset, with 39,000 articles from the Mashable website, we performed a robust rolling windows evaluation of five state of the art models. The best result was provided by a Random Forest with a discrimination power of 73{\%}. Moreover, several stochastic hill climbing local searches were explored. When optimizing 1000 articles, the best optimization method obtained a mean gain improvement of 15 percentage points in terms of the estimated popularity probability. These results attest the proposed IDSS as a valuable tool for online news authors.",
	isbn="978-3-319-23485-4"
}

@inproceedings{choosing1,
 	author = {Kotsiantis, S. B.},
 	title = {Supervised Machine Learning: A Review of Classification Techniques},
	 booktitle = {Proceedings of the 2007 Conference on Emerging Artificial Intelligence Applications in Computer Engineering: Real Word AI Systems with Applications in eHealth, HCI, Information Retrieval and Pervasive Technologies},
	 year = {2007},
	 isbn = {978-1-58603-780-2},
	 pages = {3--24},
	 numpages = {22},
	 url = {http://dl.acm.org/citation.cfm?id=1566770.1566773},
	 acmid = {1566773},
	 publisher = {IOS Press},
	 address = {Amsterdam, The Netherlands, The Netherlands},
	 keywords = {Classifiers, Data Mining, Intelligent Data Analysis, Learning Algorithms},
}

@inproceedings{choosing2,
	 author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
	 title = {An Empirical Comparison of Supervised Learning Algorithms},
	 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	 series = {ICML '06},
	 year = {2006},
	 isbn = {1-59593-383-2},
	 location = {Pittsburgh, Pennsylvania, USA},
	 pages = {161--168},
	 numpages = {8},
	 url = {http://doi.acm.org/10.1145/1143844.1143865},
	 doi = {10.1145/1143844.1143865},
	 acmid = {1143865},
	 publisher = {ACM},
	 address = {New York, NY, USA},
}

@book{data_mining_book,
 author = {Aggarwal, Charu C.},
 title = {Data Mining: The Textbook},
 year = {2015},
 isbn = {3319141414, 9783319141411},
 publisher = {Springer Publishing Company, Incorporated},
}

@article{friedman,
	title = "Stochastic gradient boosting",
	journal = "Computational Statistics and Data Analysis",
	volume = "38",
	number = "4",
	pages = "367 - 378",
	year = "2002",
	note = "Nonlinear Methods and Data Mining",
	issn = "0167-9473",
	doi = "https://doi.org/10.1016/S0167-9473(01)00065-2",
	url = "http://www.sciencedirect.com/science/article/pii/S0167947301000652",
	author = "Jerome H. Friedman",
}

@article{preprocessing,
	title = "Tutorial on practical tips of the most influential data preprocessing algorithms in data mining",
	journal = "Knowledge-Based Systems",
	volume = "98",
	pages = "1 - 29",
	year = "2016",
	issn = "0950-7051",
	doi = "https://doi.org/10.1016/j.knosys.2015.12.006",
	url = "http://www.sciencedirect.com/science/article/pii/S0950705115004785",
	author = "Salvador García and Julián Luengo and Francisco Herrera",
	keywords = "Data preprocessing, Data reduction, Missing values imputation, Noise filtering, Dimensionality reduction, Instance reduction, Discretization, Data mining"
}

@ARTICLE{gb,
  
	AUTHOR={Natekin, Alexey and Knoll, Alois},   
		 
	TITLE={Gradient boosting machines, a tutorial},      
		
	JOURNAL={Frontiers in Neurorobotics},      
		
	VOLUME={7},      

	PAGES={21},     
		
	YEAR={2013},      
		  
	URL={https://www.frontiersin.org/article/10.3389/fnbot.2013.00021},       
		
	DOI={10.3389/fnbot.2013.00021},      
		
	ISSN={1662-5218},   
	   
	ABSTRACT={Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods. A theoretical information is complemented with many descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. A set of practical examples of gradient boosting applications are presented and comprehensively analyzed.}
}

@article{gb1,
	title = "A gradient boosting approach to the Kaggle load forecasting competition",
	journal = "International Journal of Forecasting",
	volume = "30",
	number = "2",
	pages = "382 - 394",
	year = "2014",
	issn = "0169-2070",
	doi = "https://doi.org/10.1016/j.ijforecast.2013.07.005",
	url = "http://www.sciencedirect.com/science/article/pii/S0169207013000812",
	author = "Souhaib Ben Taieb and Rob J. Hyndman",
	keywords = "Short-term load forecasting, Multi-step forecasting, Additive models, Gradient boosting, Machine learning, Kaggle competition"
}

@Article{Breiman2001,
	author="Breiman, Leo",
	title="Random Forests",
	journal="Machine Learning",
	year="2001",
	month="Oct",
	day="01",
	volume="45",
	number="1",
	pages="5--32",
	abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
	issn="1573-0565",
	doi="10.1023/A:1010933404324",
	url="https://doi.org/10.1023/A:1010933404324"
}


@misc{param-opt,
	title = "\url{https://www.knime.com/nodeguide/analytics/optimization/parameter-optimization}",
	year = "Consultado el 27 de octubre",
}

@misc{smote,
	title = "\url{https://nodepit.com/node/org.knime.base.node.mine.smote.SmoteNodeFactory}",
	year = "Consultado el 28 de octubre",
}

@misc{dr,
	title = "\url{https://www.knime.com/nodeguide/analytics/preprocessing/techniques-for-dimensionality-reduction}",
	year = "Consultado el 29 de octubre",
}

@misc{dr2,
	title = "\url{https://www.knime.com/blog/seven-techniques-for-data-dimensionality-reduction}",
	year = "Consultado el 29 de octubre",
}

@misc{cf,
	title="\url{https://nodepit.com/node/org.knime.base.node.preproc.correlation.filter.CorrelationFilterNodeFactory}",
	year = "Consultado el 30 de octubre",
}

@misc{lda,
	title="\url{https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation}",
	year = "Consultado el 4 de noviembre",
}
